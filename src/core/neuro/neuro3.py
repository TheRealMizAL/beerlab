# Импортируем необходимые библиотеки
import numpy as np  # для работы с массивами и математическими операциями
import pandas as pd  # для работы с csv файлом

# Загружаем данные из csv файла
data = pd.read_csv('beers222.csv')
# Переводим данные в numpy массивы
X = data[['ABV', 'IBU', 'SRM']].to_numpy()  # входные данные - два столбца с ABV, IBU и SRM
y = data['Style'].to_numpy()  # выходные данные - один столбец со стилем пива
# Преобразуем выходные данные в one-hot encoding
# Сорта пива кодируются целыми числами от 0 до 4
y = np.eye(5)[y]  # создаем единичную матрицу размера 5 и выбираем строки по индексам из y

# Определяем параметры нейросети
n_input = 3  # количество входных нейронов
n_hidden = 6  # количество скрытых нейронов
n_output = 5  # количество выходных нейронов
# Инициализируем веса и отклонения случайными значениями из нормального распределения
W1 = np.random.randn(n_input, n_hidden)  # матрица весов между входным и скрытым слоем
b1 = np.random.randn(n_hidden)  # вектор отклонений для скрытого слоя
W2 = np.random.randn(n_hidden, n_output)  # матрица весов между скрытым и выходным слоем
b2 = np.random.randn(n_output)  # вектор отклонений для выходного слоя


# Определяем функции активации и их производные
def relu(x):  # функция ReLu
    return np.maximum(0, x)  # возвращает максимум между 0 и x


def relu_derivative(x):  # производная функции ReLu
    return (x > 0).astype(float)  # возвращает 1, если x больше 0, иначе 0


def softmax(x):  # функция softmax
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # вычитаем максимум по строкам для численной стабилизации
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)  # делим на сумму по строкам


def cross_entropy(y_true, y_pred):  # функция потерь - кросс-энтропия
    return -np.mean(
        y_true * np.log(y_pred))  # возвращает отрицательное среднее произведения истинных и предсказанных значений


# Определяем гиперпараметры обучения
epochs = 100000  # количество эпох обучения
alpha = 0.00001  # скорость обучения

# Начинаем обучение нейросети
for epoch in range(epochs):  # повторяем для каждой эпохи
    # Прямое распространение (forward propagation)
    h1 = X @ W1 + b1  # линейная комбинация входов и весов для скрытого слоя
    t1 = relu(h1)  # активация скрытого слоя функцией ReLu
    h2 = t1 @ W2 + b2  # линейная комбинация скрытых нейронов и весов для выходного слоя
    z = softmax(h2)  # активация выходного слоя функцией softmax
    # Обратное распространение ошибки
    dE_dh2 = z - y  # градиент ошибки по h2
    dE_dW2 = t1.T @ dE_dh2  # градиент ошибки по W2
    dE_db2 = np.sum(dE_dh2, axis=0)  # градиент ошибки по b2
    dE_dt1 = dE_dh2 @ W2.T  # градиент ошибки по t1
    dE_dh1 = dE_dt1 * relu_derivative(h1)  # градиент ошибки по h1
    dE_dW1 = X.T @ dE_dh1  # градиент ошибки по W1
    dE_db1 = np.sum(dE_dh1, axis=0)  # градиент ошибки по b1
    # Обновление весов и отклонений
    W2 = W2 - alpha * dE_dW2  # вычитаем произведение скорости обучения и градиента по W2
    b2 = b2 - alpha * dE_db2  # вычитаем произведение скорости обучения и градиента по b2
    W1 = W1 - alpha * dE_dW1  # вычитаем произведение скорости обучения и градиента по W1
    b1 = b1 - alpha * dE_db1  # вычитаем произведение скорости обучения и градиента по b1
    # Вычисляем значение функции потерь
    loss = cross_entropy(y, z)  # вызываем функцию кросс-энтропии с истинными и предсказанными значениями
    # Выводим информацию о процессе обучения
    # Выводим значение функции потерь на каждой 100-й эпохе
    if epoch % 100 == 0:
        print(
            f'Эпоха {epoch + 100}, значение функции потерь: {loss:.6f}')  # печатаем номер эпохи и значение функции потерь
print('W1', W1)
print('b1', b1)
print('W2', W2)
print('b2', b2)
